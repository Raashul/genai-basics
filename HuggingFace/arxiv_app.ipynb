{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArXiv Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/hello/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hello/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/hello/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/hello/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /opt/anaconda3/envs/hello/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/hello/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/hello/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/hello/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/hello/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/2vgzh3mn7fj69blg01hsgt380000gn/T/ipykernel_82816/3709015367.py:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-06 18:59:48+00:00</td>\n",
       "      <td>L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling</td>\n",
       "      <td>We rigorously establish a bipartite mutual information scaling law in natural\\nlanguage that governs long-range dependencies. This scaling law, which we show\\nis distinct from and scales independently of the conventional two-point mutual\\ninformation, is the key to understanding long-context language modeling. Using\\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\\ncondition, which relates a model's capacity for effective long context length\\nmodeling to the scaling of its latent state size for storing past information.\\nOur results are validated through experiments on both transformers and state\\nspace models. This work establishes a theoretical foundation that guides the\\ndevelopment of large language models toward longer context lengths.</td>\n",
       "      <td>[cs.CL, cs.AI, cs.IT, cs.LG, math.IT, physics.data-an]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-06 18:59:37+00:00</td>\n",
       "      <td>Shifting Long-Context LLMs Research from Input to Output</td>\n",
       "      <td>Recent advancements in long-context Large Language Models (LLMs) have\\nprimarily concentrated on processing extended input contexts, resulting in\\nsignificant strides in long-context comprehension. However, the equally\\ncritical aspect of generating long-form outputs has received comparatively less\\nattention. This paper advocates for a paradigm shift in NLP research toward\\naddressing the challenges of long-output generation. Tasks such as novel\\nwriting, long-term planning, and complex reasoning require models to understand\\nextensive contexts and produce coherent, contextually rich, and logically\\nconsistent extended text. These demands highlight a critical gap in current LLM\\ncapabilities. We underscore the importance of this under-explored domain and\\ncall for focused efforts to develop foundational LLMs tailored for generating\\nhigh-quality, long-form outputs, which hold immense potential for real-world\\napplications.</td>\n",
       "      <td>[cs.CL, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-06 18:59:23+00:00</td>\n",
       "      <td>Enough Coin Flips Can Make LLMs Act Bayesian</td>\n",
       "      <td>Large language models (LLMs) exhibit the ability to generalize given few-shot\\nexamples in their input prompt, an emergent capability known as in-context\\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\\nreasoning in ways that are consistent with a Bayesian framework or rely on\\npattern matching. Using a controlled setting of biased coin flips, we find\\nthat: (1) LLMs often possess biased priors, causing initial divergence in\\nzero-shot settings, (2) in-context evidence outweighs explicit bias\\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\\ndeviations primarily due to miscalibrated priors rather than flawed updates,\\nand (4) attention magnitude has negligible effect on Bayesian inference. With\\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\\npriors in a Bayesian manner.</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-06 18:58:45+00:00</td>\n",
       "      <td>Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation</td>\n",
       "      <td>Scene flow estimation is a foundational task for many robotic applications,\\nincluding robust dynamic object detection, automatic labeling, and sensor\\nsynchronization. Two types of approaches to the problem have evolved: 1)\\nSupervised and 2) optimization-based methods. Supervised methods are fast\\nduring inference and achieve high-quality results, however, they are limited by\\nthe need for large amounts of labeled training data and are susceptible to\\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\\nface the problem of domain gaps but usually suffer from substantial runtime,\\nexhibit artifacts, or fail to converge to the right solution. In this work, we\\nmitigate several limitations of existing optimization-based methods. To this\\nend, we 1) introduce a simple voxel grid-based model that improves over the\\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\\nmultiframe loss formulation. 3) We combine both contributions in our new\\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\\nby EulerFlow among unsupervised methods while achieving comparable performance\\nat a fraction of the computational cost. Floxels achieves a massive speedup of\\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\\nachieves a speedup of ~14x.</td>\n",
       "      <td>[cs.CV, cs.LG, cs.RO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-06 18:58:29+00:00</td>\n",
       "      <td>Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining</td>\n",
       "      <td>The impressive capabilities of Large Language Models (LLMs) across diverse\\ntasks are now well-established, yet their effective deployment necessitates\\ncareful hyperparameter optimization. Through extensive empirical studies\\ninvolving grid searches across diverse configurations, we discover universal\\nscaling laws governing these hyperparameters: optimal learning rate follows a\\npower-law relationship with both model parameters and data sizes, while optimal\\nbatch size scales primarily with data sizes. Our analysis reveals a convex\\noptimization landscape for hyperparameters under fixed models and data size\\nconditions. This convexity implies an optimal hyperparameter plateau. We\\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\\nglobally optimal LLM performance found via an exhaustive search. These laws\\ndemonstrate remarkable robustness across variations in model sparsity, training\\ndata distribution, and model shape. To our best known, this is the first work\\nthat unifies different model shapes and structures, such as Mixture-of-Experts\\nmodels and dense transformers, as well as establishes optimal hyperparameter\\nscaling laws across diverse data distributions. This exhaustive optimization\\nprocess demands substantial computational resources, utilizing nearly one\\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\\ntotal. To facilitate reproducibility and further research, we will\\nprogressively release all loss measurements and model checkpoints through our\\ndesignated repository https://step-law.github.io/</td>\n",
       "      <td>[cs.LG, cs.AI, F.2.2; I.2.7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-03-06 18:57:40+00:00</td>\n",
       "      <td>Scaling Rich Style-Prompted Text-to-Speech Datasets</td>\n",
       "      <td>We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale\\ndataset that annotates speech utterances with rich style captions. While rich\\nabstract tags (e.g. guttural, nasal, pained) have been explored in small-scale\\nhuman-annotated datasets, existing large-scale datasets only cover basic tags\\n(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech\\nembedders, classifiers and an audio language model to automatically scale rich\\ntag annotations for the first time. ParaSpeechCaps covers a total of 59 style\\ntags, including both speaker-level intrinsic tags and utterance-level\\nsituational tags. It consists of 342 hours of human-labelled data (PSC-Base)\\nand 2427 hours of automatically annotated data (PSC-Scaled). We finetune\\nParler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and\\nachieve improved style consistency (+7.9% Consistency MOS) and speech quality\\n(+15.5% Naturalness MOS) over the best performing baseline that combines\\nexisting rich style tag datasets. We ablate several of our dataset design\\nchoices to lay the foundation for future work in this space. Our dataset,\\nmodels and code are released at https://github.com/ajd12342/paraspeechcaps .</td>\n",
       "      <td>[eess.AS, cs.AI, cs.CL, cs.LG, cs.SD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-03-06 18:57:34+00:00</td>\n",
       "      <td>Efficiently Escaping Saddle Points under Generalized Smoothness via Self-Bounding Regularity</td>\n",
       "      <td>In this paper, we study the problem of non-convex optimization on functions\\nthat are not necessarily smooth using first order methods. Smoothness\\n(functions whose gradient and/or Hessian are Lipschitz) is not satisfied by\\nmany machine learning problems in both theory and practice, motivating a recent\\nline of work studying the convergence of first order methods to first order\\nstationary points under appropriate generalizations of smoothness.\\n  We develop a novel framework to study convergence of first order methods to\\nfirst and \\textit{second} order stationary points under generalized smoothness,\\nunder more general smoothness assumptions than the literature. Using our\\nframework, we show appropriate variants of GD and SGD (e.g. with appropriate\\nperturbations) can converge not just to first order but also \\textit{second\\norder stationary points} in runtime polylogarithmic in the dimension. To our\\nknowledge, our work contains the first such result, as well as the first\\n'non-textbook' rate for non-convex optimization under generalized smoothness.\\nWe demonstrate that several canonical non-convex optimization problems fall\\nunder our setting and framework.</td>\n",
       "      <td>[math.OC, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-03-06 18:57:16+00:00</td>\n",
       "      <td>Self-Supervised Models for Phoneme Recognition: Applications in Children's Speech for Reading Learning</td>\n",
       "      <td>Child speech recognition is still an underdeveloped area of research due to\\nthe lack of data (especially on non-English languages) and the specific\\ndifficulties of this task. Having explored various architectures for child\\nspeech recognition in previous work, in this article we tackle recent\\nself-supervised models. We first compare wav2vec 2.0, HuBERT and WavLM models\\nadapted to phoneme recognition in French child speech, and continue our\\nexperiments with the best of them, WavLM base+. We then further adapt it by\\nunfreezing its transformer blocks during fine-tuning on child speech, which\\ngreatly improves its performance and makes it significantly outperform our base\\nmodel, a Transformer+CTC. Finally, we study in detail the behaviour of these\\ntwo models under the real conditions of our application, and show that WavLM\\nbase+ is more robust to various reading tasks and noise levels. Index Terms:\\nspeech recognition, child speech, self-supervised learning</td>\n",
       "      <td>[cs.SD, cs.AI, eess.AS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-03-06 18:54:42+00:00</td>\n",
       "      <td>Sample-Optimal Agnostic Boosting with Unlabeled Data</td>\n",
       "      <td>Boosting provides a practical and provably effective framework for\\nconstructing accurate learning algorithms from inaccurate rules of thumb. It\\nextends the promise of sample-efficient learning to settings where direct\\nEmpirical Risk Minimization (ERM) may not be implementable efficiently. In the\\nrealizable setting, boosting is known to offer this computational reprieve\\nwithout compromising on sample efficiency. However, in the agnostic case,\\nexisting boosting algorithms fall short of achieving the optimal sample\\ncomplexity.\\n  This paper highlights an unexpected and previously unexplored avenue of\\nimprovement: unlabeled samples. We design a computationally efficient agnostic\\nboosting algorithm that matches the sample complexity of ERM, given\\npolynomially many additional unlabeled samples. In fact, we show that the total\\nnumber of samples needed, unlabeled and labeled inclusive, is never more than\\nthat for the best known agnostic boosting algorithm -- so this result is never\\nworse -- while only a vanishing fraction of these need to be labeled for the\\nalgorithm to succeed. This is particularly fortuitous for learning-theoretic\\napplications of agnostic boosting, which often take place in the\\ndistribution-specific setting, where unlabeled samples can be availed for free.\\nWe detail other applications of this result in reinforcement learning.</td>\n",
       "      <td>[cs.LG, stat.ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-03-06 18:54:32+00:00</td>\n",
       "      <td>Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size</td>\n",
       "      <td>We present a novel approach to selective model quantization that transcends\\nthe limitations of architecture-specific and size-dependent compression methods\\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\\nanalyzing the entropy distribution across transformer blocks, EWQ determines\\nwhich blocks can be safely quantized without causing significant performance\\ndegradation, independent of model architecture or size. Our method outperforms\\nuniform quantization approaches, maintaining Massive Multitask Language\\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\\nacross multiple architectures -- from 1.6B to 70B parameters -- and showcase\\nconsistent improvements in the quality-compression trade-off regardless of\\nmodel scale or architectural design. A surprising finding of EWQ is its ability\\nto reduce perplexity compared to unquantized models, suggesting the presence of\\nbeneficial regularization through selective precision reduction. This\\nimprovement holds across different model families, indicating a fundamental\\nrelationship between layer-level entropy and optimal precision requirements.\\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\\nanalysis that eliminates the need for loading model weights. This technique\\nleverages universal characteristics of entropy distribution that persist across\\nvarious architectures and scales, enabling near-instantaneous quantization\\ndecisions while maintaining 80% classification accuracy with full entropy\\nanalysis. Our results demonstrate that effective quantization strategies can be\\ndeveloped independently of specific architectural choices or model sizes,\\nopening new possibilities for efficient LLM deployment.</td>\n",
       "      <td>[cs.LG, cs.AI]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published  \\\n",
       "0 2025-03-06 18:59:48+00:00   \n",
       "1 2025-03-06 18:59:37+00:00   \n",
       "2 2025-03-06 18:59:23+00:00   \n",
       "3 2025-03-06 18:58:45+00:00   \n",
       "4 2025-03-06 18:58:29+00:00   \n",
       "5 2025-03-06 18:57:40+00:00   \n",
       "6 2025-03-06 18:57:34+00:00   \n",
       "7 2025-03-06 18:57:16+00:00   \n",
       "8 2025-03-06 18:54:42+00:00   \n",
       "9 2025-03-06 18:54:32+00:00   \n",
       "\n",
       "                                                                                                    title  \\\n",
       "0                               L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling   \n",
       "1                                                Shifting Long-Context LLMs Research from Input to Output   \n",
       "2                                                            Enough Coin Flips Can Make LLMs Act Bayesian   \n",
       "3                                            Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation   \n",
       "4     Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining   \n",
       "5                                                     Scaling Rich Style-Prompted Text-to-Speech Datasets   \n",
       "6            Efficiently Escaping Saddle Points under Generalized Smoothness via Self-Bounding Regularity   \n",
       "7  Self-Supervised Models for Phoneme Recognition: Applications in Children's Speech for Reading Learning   \n",
       "8                                                    Sample-Optimal Agnostic Boosting with Unlabeled Data   \n",
       "9            Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            abstract  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We rigorously establish a bipartite mutual information scaling law in natural\\nlanguage that governs long-range dependencies. This scaling law, which we show\\nis distinct from and scales independently of the conventional two-point mutual\\ninformation, is the key to understanding long-context language modeling. Using\\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\\ncondition, which relates a model's capacity for effective long context length\\nmodeling to the scaling of its latent state size for storing past information.\\nOur results are validated through experiments on both transformers and state\\nspace models. This work establishes a theoretical foundation that guides the\\ndevelopment of large language models toward longer context lengths.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Recent advancements in long-context Large Language Models (LLMs) have\\nprimarily concentrated on processing extended input contexts, resulting in\\nsignificant strides in long-context comprehension. However, the equally\\ncritical aspect of generating long-form outputs has received comparatively less\\nattention. This paper advocates for a paradigm shift in NLP research toward\\naddressing the challenges of long-output generation. Tasks such as novel\\nwriting, long-term planning, and complex reasoning require models to understand\\nextensive contexts and produce coherent, contextually rich, and logically\\nconsistent extended text. These demands highlight a critical gap in current LLM\\ncapabilities. We underscore the importance of this under-explored domain and\\ncall for focused efforts to develop foundational LLMs tailored for generating\\nhigh-quality, long-form outputs, which hold immense potential for real-world\\napplications.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Large language models (LLMs) exhibit the ability to generalize given few-shot\\nexamples in their input prompt, an emergent capability known as in-context\\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\\nreasoning in ways that are consistent with a Bayesian framework or rely on\\npattern matching. Using a controlled setting of biased coin flips, we find\\nthat: (1) LLMs often possess biased priors, causing initial divergence in\\nzero-shot settings, (2) in-context evidence outweighs explicit bias\\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\\ndeviations primarily due to miscalibrated priors rather than flawed updates,\\nand (4) attention magnitude has negligible effect on Bayesian inference. With\\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\\npriors in a Bayesian manner.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                   Scene flow estimation is a foundational task for many robotic applications,\\nincluding robust dynamic object detection, automatic labeling, and sensor\\nsynchronization. Two types of approaches to the problem have evolved: 1)\\nSupervised and 2) optimization-based methods. Supervised methods are fast\\nduring inference and achieve high-quality results, however, they are limited by\\nthe need for large amounts of labeled training data and are susceptible to\\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\\nface the problem of domain gaps but usually suffer from substantial runtime,\\nexhibit artifacts, or fail to converge to the right solution. In this work, we\\nmitigate several limitations of existing optimization-based methods. To this\\nend, we 1) introduce a simple voxel grid-based model that improves over the\\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\\nmultiframe loss formulation. 3) We combine both contributions in our new\\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\\nby EulerFlow among unsupervised methods while achieving comparable performance\\nat a fraction of the computational cost. Floxels achieves a massive speedup of\\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\\nachieves a speedup of ~14x.   \n",
       "4                                                                                                  The impressive capabilities of Large Language Models (LLMs) across diverse\\ntasks are now well-established, yet their effective deployment necessitates\\ncareful hyperparameter optimization. Through extensive empirical studies\\ninvolving grid searches across diverse configurations, we discover universal\\nscaling laws governing these hyperparameters: optimal learning rate follows a\\npower-law relationship with both model parameters and data sizes, while optimal\\nbatch size scales primarily with data sizes. Our analysis reveals a convex\\noptimization landscape for hyperparameters under fixed models and data size\\nconditions. This convexity implies an optimal hyperparameter plateau. We\\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\\nglobally optimal LLM performance found via an exhaustive search. These laws\\ndemonstrate remarkable robustness across variations in model sparsity, training\\ndata distribution, and model shape. To our best known, this is the first work\\nthat unifies different model shapes and structures, such as Mixture-of-Experts\\nmodels and dense transformers, as well as establishes optimal hyperparameter\\nscaling laws across diverse data distributions. This exhaustive optimization\\nprocess demands substantial computational resources, utilizing nearly one\\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\\ntotal. To facilitate reproducibility and further research, we will\\nprogressively release all loss measurements and model checkpoints through our\\ndesignated repository https://step-law.github.io/   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale\\ndataset that annotates speech utterances with rich style captions. While rich\\nabstract tags (e.g. guttural, nasal, pained) have been explored in small-scale\\nhuman-annotated datasets, existing large-scale datasets only cover basic tags\\n(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech\\nembedders, classifiers and an audio language model to automatically scale rich\\ntag annotations for the first time. ParaSpeechCaps covers a total of 59 style\\ntags, including both speaker-level intrinsic tags and utterance-level\\nsituational tags. It consists of 342 hours of human-labelled data (PSC-Base)\\nand 2427 hours of automatically annotated data (PSC-Scaled). We finetune\\nParler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and\\nachieve improved style consistency (+7.9% Consistency MOS) and speech quality\\n(+15.5% Naturalness MOS) over the best performing baseline that combines\\nexisting rich style tag datasets. We ablate several of our dataset design\\nchoices to lay the foundation for future work in this space. Our dataset,\\nmodels and code are released at https://github.com/ajd12342/paraspeechcaps .   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In this paper, we study the problem of non-convex optimization on functions\\nthat are not necessarily smooth using first order methods. Smoothness\\n(functions whose gradient and/or Hessian are Lipschitz) is not satisfied by\\nmany machine learning problems in both theory and practice, motivating a recent\\nline of work studying the convergence of first order methods to first order\\nstationary points under appropriate generalizations of smoothness.\\n  We develop a novel framework to study convergence of first order methods to\\nfirst and \\textit{second} order stationary points under generalized smoothness,\\nunder more general smoothness assumptions than the literature. Using our\\nframework, we show appropriate variants of GD and SGD (e.g. with appropriate\\nperturbations) can converge not just to first order but also \\textit{second\\norder stationary points} in runtime polylogarithmic in the dimension. To our\\nknowledge, our work contains the first such result, as well as the first\\n'non-textbook' rate for non-convex optimization under generalized smoothness.\\nWe demonstrate that several canonical non-convex optimization problems fall\\nunder our setting and framework.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Child speech recognition is still an underdeveloped area of research due to\\nthe lack of data (especially on non-English languages) and the specific\\ndifficulties of this task. Having explored various architectures for child\\nspeech recognition in previous work, in this article we tackle recent\\nself-supervised models. We first compare wav2vec 2.0, HuBERT and WavLM models\\nadapted to phoneme recognition in French child speech, and continue our\\nexperiments with the best of them, WavLM base+. We then further adapt it by\\nunfreezing its transformer blocks during fine-tuning on child speech, which\\ngreatly improves its performance and makes it significantly outperform our base\\nmodel, a Transformer+CTC. Finally, we study in detail the behaviour of these\\ntwo models under the real conditions of our application, and show that WavLM\\nbase+ is more robust to various reading tasks and noise levels. Index Terms:\\nspeech recognition, child speech, self-supervised learning   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Boosting provides a practical and provably effective framework for\\nconstructing accurate learning algorithms from inaccurate rules of thumb. It\\nextends the promise of sample-efficient learning to settings where direct\\nEmpirical Risk Minimization (ERM) may not be implementable efficiently. In the\\nrealizable setting, boosting is known to offer this computational reprieve\\nwithout compromising on sample efficiency. However, in the agnostic case,\\nexisting boosting algorithms fall short of achieving the optimal sample\\ncomplexity.\\n  This paper highlights an unexpected and previously unexplored avenue of\\nimprovement: unlabeled samples. We design a computationally efficient agnostic\\nboosting algorithm that matches the sample complexity of ERM, given\\npolynomially many additional unlabeled samples. In fact, we show that the total\\nnumber of samples needed, unlabeled and labeled inclusive, is never more than\\nthat for the best known agnostic boosting algorithm -- so this result is never\\nworse -- while only a vanishing fraction of these need to be labeled for the\\nalgorithm to succeed. This is particularly fortuitous for learning-theoretic\\napplications of agnostic boosting, which often take place in the\\ndistribution-specific setting, where unlabeled samples can be availed for free.\\nWe detail other applications of this result in reinforcement learning.   \n",
       "9  We present a novel approach to selective model quantization that transcends\\nthe limitations of architecture-specific and size-dependent compression methods\\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\\nanalyzing the entropy distribution across transformer blocks, EWQ determines\\nwhich blocks can be safely quantized without causing significant performance\\ndegradation, independent of model architecture or size. Our method outperforms\\nuniform quantization approaches, maintaining Massive Multitask Language\\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\\nacross multiple architectures -- from 1.6B to 70B parameters -- and showcase\\nconsistent improvements in the quality-compression trade-off regardless of\\nmodel scale or architectural design. A surprising finding of EWQ is its ability\\nto reduce perplexity compared to unquantized models, suggesting the presence of\\nbeneficial regularization through selective precision reduction. This\\nimprovement holds across different model families, indicating a fundamental\\nrelationship between layer-level entropy and optimal precision requirements.\\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\\nanalysis that eliminates the need for loading model weights. This technique\\nleverages universal characteristics of entropy distribution that persist across\\nvarious architectures and scales, enabling near-instantaneous quantization\\ndecisions while maintaining 80% classification accuracy with full entropy\\nanalysis. Our results demonstrate that effective quantization strategies can be\\ndeveloped independently of specific architectural choices or model sizes,\\nopening new possibilities for efficient LLM deployment.   \n",
       "\n",
       "                                               categories  \n",
       "0  [cs.CL, cs.AI, cs.IT, cs.LG, math.IT, physics.data-an]  \n",
       "1                                          [cs.CL, cs.AI]  \n",
       "2                                   [cs.CL, cs.AI, cs.LG]  \n",
       "3                                   [cs.CV, cs.LG, cs.RO]  \n",
       "4                            [cs.LG, cs.AI, F.2.2; I.2.7]  \n",
       "5                   [eess.AS, cs.AI, cs.CL, cs.LG, cs.SD]  \n",
       "6                                        [math.OC, cs.LG]  \n",
       "7                                 [cs.SD, cs.AI, eess.AS]  \n",
       "8                                        [cs.LG, stat.ML]  \n",
       "9                                          [cs.LG, cs.AI]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query to fetch AI-related papers\n",
    "query = 'ai OR artificial intelligence OR machine learning'\n",
    "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
    "\n",
    "# Fetch papers\n",
    "papers = []\n",
    "for result in search.results():\n",
    "    papers.append({\n",
    "      'published': result.published,\n",
    "        'title': result.title,\n",
    "        'abstract': result.summary,\n",
    "        'categories': result.categories\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Example abstract from API\n",
    "abstract = df['abstract'][0]\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Summarization\n",
    "summarization_result = summarizer(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We rigorously establish a bipartite mutual information scaling law in naturallanguage that governs long-range dependencies. This scaling law is the key to understanding long-context language modeling. This work establishes a theoretical foundation that guides thedevelopment of large language models toward longer context lengths.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summarization_result[0]['summary_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
